{
 "cells": [
  {
   "cell_type": "raw",
   "id": "12ecfc45",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"SimCLR: Simple Contrastive Learning of Visual Representations\"\n",
    "date: \"2025-07-12\"\n",
    "categories: [self-supervised, contrastive-learning, vision]\n",
    "badges: true\n",
    "description: An in-depth look at SimCLR, a self-supervised learning framework that leverages contrastive learning to learn visual representations without labeled data.\n",
    "output-file: 2025-07-12-SimCLR.html\n",
    "author: Jeet Parab\n",
    "toc: true\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5567af0c",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "SimCLR (Simple Contrastive Learning of Visual Representations) is a self-supervised learning framework for learning visual representations without labels. It was introduced by Chen et al. in 2020 and has become one of the most influential methods in contrastive learning.\n",
    "\n",
    "::: {.callout-note}\n",
    "**Read the original paper:**  **Chen, Ting, et al.**  \n",
    "*A Simple Framework for Contrastive Learning of Visual Representations* (2020)  \n",
    "[arXiv:2002.05709](https://arxiv.org/abs/2002.05709)\n",
    ":::\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Contrastive Learning**: Learn representations by contrasting positive and negative examples.\n",
    "2. **Data Augmentation**: Create positive pairs through augmentation of the same image.\n",
    "3. **Projection Head**: Use a non-linear projection head during training.\n",
    "4. **Large Batch Sizes**: Utilize large batch sizes for more negative examples.\n",
    "\n",
    "### How SimCLR Works\n",
    "\n",
    "1. Take a batch of images.\n",
    "2. Apply two different augmentations to each image (creating positive pairs).\n",
    "3. Pass augmented images through an encoder (e.g., ResNet).\n",
    "4. Apply a projection head to get representations.\n",
    "5. Use contrastive loss (NT-Xent) to pull positive pairs together and push negative pairs apart.\n",
    "\n",
    "![SimCLR Architecture Overview](/assets/Simclr.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66c257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9a8154",
   "metadata": {},
   "source": [
    "## Data Augmentation Pipeline\n",
    "\n",
    "Data augmentation is crucial for SimCLR's success. The framework uses a composition of augmentations to create positive pairs from the same image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccb4156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation Pipeline\n",
    "class SimCLRTransform:\n",
    "    \n",
    "    def __init__(self, image_size=224, s=1.0):\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # Color distortion\n",
    "        color_jitter = transforms.ColorJitter(\n",
    "            brightness=0.8 * s,\n",
    "            contrast=0.8 * s,\n",
    "            saturation=0.8 * s,\n",
    "            hue=0.2 * s\n",
    "        )\n",
    "        \n",
    "        # SimCLR augmentation pipeline\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(image_size, scale=(0.08, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomApply([color_jitter], p=0.8),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Return two augmented versions of the same image\n",
    "        return self.transform(x), self.transform(x)\n",
    "\n",
    "# Demonstration of augmentation\n",
    "transform = SimCLRTransform()\n",
    "print(\"SimCLR augmentation pipeline created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9725ddd4",
   "metadata": {},
   "source": [
    "## SimCLR Model Architecture \n",
    "\n",
    "The SimCLR model is built from two key components:\n",
    "\n",
    "### 1. Encoder\n",
    "\n",
    "- A deep convolutional neural network (CNN), such as ResNet, is used to extract feature representations from input images.\n",
    "- The encoder learns to map images to a high-dimensional feature space that captures semantic content.\n",
    "\n",
    "### 2. Projection Head\n",
    "\n",
    "- A small multilayer perceptron (MLP) that takes the encoder’s output and projects it into a lower-dimensional space.\n",
    "- This projection is where the contrastive loss is applied, encouraging similar images (positive pairs) to have similar representations and dissimilar images (negative pairs) to be far apart.\n",
    "\n",
    "\n",
    "### Key Points\n",
    "\n",
    "- The projection head is used only during contrastive pre-training; for downstream tasks, only the encoder is retained.\n",
    "- This separation improves the quality of learned representations and makes SimCLR simple yet powerful for self-supervised learning.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f69f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimCLR Model Architecture\n",
    "class ProjectionHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim=512, hidden_dim=512, output_dim=128):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.projection(x)\n",
    "\n",
    "class SimCLR(nn.Module):\n",
    "    \"\"\"SimCLR model implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, base_encoder='resnet18', projection_dim=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Base encoder\n",
    "        if base_encoder == 'resnet18':\n",
    "            self.encoder = torchvision.models.resnet18(weights=None)\n",
    "            self.encoder.fc = nn.Identity()  # Remove classification head\n",
    "            encoder_dim = 512\n",
    "        elif base_encoder == 'resnet50':\n",
    "            self.encoder = torchvision.models.resnet50(weights=None)\n",
    "            self.encoder.fc = nn.Identity()\n",
    "            encoder_dim = 2048\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported encoder: {base_encoder}\")\n",
    "        \n",
    "        # Projection head\n",
    "        self.projection_head = ProjectionHead(\n",
    "            input_dim=encoder_dim,\n",
    "            output_dim=projection_dim\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract features\n",
    "        features = self.encoder(x)\n",
    "        # Project features\n",
    "        projections = self.projection_head(features)\n",
    "        return features, projections\n",
    "\n",
    "# Create model\n",
    "model = SimCLR(base_encoder='resnet18', projection_dim=128)\n",
    "model = model.to(device)\n",
    "print(f\"SimCLR model created with {sum(p.numel() for p in model.parameters())} parameters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4ae709",
   "metadata": {},
   "source": [
    "## NT-Xent Loss Function\n",
    "\n",
    "The Normalized Temperature-scaled Cross-Entropy (NT-Xent) loss is the heart of SimCLR. It encourages similar representations for positive pairs while pushing apart negative pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2899adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NT-Xent Loss Function\n",
    "class NTXentLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "        self.similarity_f = nn.CosineSimilarity(dim=2)\n",
    "    \n",
    "    def mask_correlated_samples(self, batch_size):\n",
    "        \"\"\"Create mask to remove self-similarity and correlated samples\"\"\"\n",
    "        N = 2 * batch_size\n",
    "        mask = torch.ones((N, N), dtype=bool)\n",
    "        mask = mask.fill_diagonal_(0)\n",
    "        for i in range(batch_size):\n",
    "            mask[i, batch_size + i] = 0\n",
    "            mask[batch_size + i, i] = 0\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, z_i, z_j):\n",
    "        \"\"\"Calculate NT-Xent loss\"\"\"\n",
    "        batch_size = z_i.shape[0]\n",
    "        N = 2 * batch_size\n",
    "        z = torch.cat((z_i, z_j), dim=0)\n",
    "        sim = self.similarity_f(z.unsqueeze(1), z.unsqueeze(0)) / self.temperature\n",
    "        sim_i_j = torch.diag(sim, batch_size)\n",
    "        sim_j_i = torch.diag(sim, -batch_size)\n",
    "        positive_samples = torch.cat((sim_i_j, sim_j_i), dim=0).reshape(N, 1)\n",
    "        mask = self.mask_correlated_samples(batch_size)\n",
    "        negative_samples = sim[mask].reshape(N, -1)\n",
    "        labels = torch.zeros(N).to(positive_samples.device).long()\n",
    "        logits = torch.cat((positive_samples, negative_samples), dim=1)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        return loss / N\n",
    "\n",
    "# Create loss function\n",
    "criterion = NTXentLoss(temperature=0.07)\n",
    "print(\"NT-Xent loss function created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895c2481",
   "metadata": {},
   "source": [
    "## Custom Dataset for SimCLR\n",
    "\n",
    "We'll create a custom dataset class that applies SimCLR augmentations to create positive pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265c865b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset for SimCLR\n",
    "class SimCLRDataset(Dataset):\n",
    "    \"\"\"Dataset wrapper for SimCLR that returns augmented pairs\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image, _ = self.dataset[idx]  \n",
    "        # Note that labels are ignored for self supervised learning\n",
    "        if self.transform:\n",
    "            aug1, aug2 = self.transform(image)\n",
    "            return aug1, aug2\n",
    "        else:\n",
    "            return image, image\n",
    "\n",
    "# Load CIFAR-10 dataset (you can replace with your own dataset)\n",
    "base_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToPILImage()\n",
    ")\n",
    "\n",
    "# Create SimCLR dataset\n",
    "simclr_dataset = SimCLRDataset(base_dataset, transform=SimCLRTransform(image_size=32))\n",
    "dataloader = DataLoader(simclr_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "print(f\"Dataset created with {len(simclr_dataset)} samples\")\n",
    "print(f\"Dataloader created with batch size {dataloader.batch_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b32e1e",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Now let's implement the training loop for SimCLR. This demonstrates how the model learns representations through contrastive learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9f0d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "def train_simclr(model, dataloader, criterion, optimizer, num_epochs=25):\n",
    "    \"\"\"Training loop for SimCLR\"\"\"\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for batch_idx, (aug1, aug2) in enumerate(dataloader):\n",
    "            aug1, aug2 = aug1.to(device), aug2.to(device)\n",
    "            _, z1 = model(aug1)\n",
    "            _, z2 = model(aug2)\n",
    "            z1 = F.normalize(z1, dim=1)\n",
    "            z2 = F.normalize(z2, dim=1)\n",
    "            loss = criterion(z1, z2)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(dataloader)}, Loss: {loss.item():.4f}')\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        losses.append(avg_loss)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} completed. Average Loss: {avg_loss:.4f}')\n",
    "    return losses\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-6)\n",
    "\n",
    "# Train the model \n",
    "print(\"Starting SimCLR training\")\n",
    "losses = train_simclr(model, dataloader, criterion, optimizer, num_epochs=25)\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses)\n",
    "plt.title('SimCLR Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Training completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fe413d",
   "metadata": {},
   "source": [
    "## Evaluation: Linear Probing\n",
    "\n",
    "After pre-training with SimCLR, we typically evaluate the learned representations using linear probing—training a linear classifier on top of frozen features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe3faf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation: Linear Probing\n",
    "class LinearProbe(nn.Module):\n",
    "    \"\"\"Linear classifier for evaluation\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Linear(feature_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "\n",
    "def evaluate_linear_probe(model, train_loader, test_loader, num_classes=10):\n",
    "    \"\"\"Evaluate learned representations using linear probing\"\"\"\n",
    "    model.eval()\n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            features, _ = model(images)\n",
    "            train_features.append(features.cpu())\n",
    "            train_labels.append(labels)\n",
    "    train_features = torch.cat(train_features)\n",
    "    train_labels = torch.cat(train_labels)\n",
    "    linear_probe = LinearProbe(train_features.shape[1], num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(linear_probe.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    linear_probe.train()\n",
    "    for epoch in range(10):\n",
    "        indices = torch.randperm(len(train_features))\n",
    "        for i in range(0, len(train_features), 256):\n",
    "            batch_indices = indices[i:i+256]\n",
    "            batch_features = train_features[batch_indices].to(device)\n",
    "            batch_labels = train_labels[batch_indices].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = linear_probe(batch_features)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    linear_probe.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            features, _ = model(images)\n",
    "            outputs = linear_probe(features)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted.cpu() == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Create evaluation datasets\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_eval_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=False, transform=eval_transform\n",
    ")\n",
    "test_eval_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=False, transform=eval_transform\n",
    ")\n",
    "\n",
    "train_eval_loader = DataLoader(train_eval_dataset, batch_size=256, shuffle=False)\n",
    "test_eval_loader = DataLoader(test_eval_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Evaluating learned representations\")\n",
    "accuracy = evaluate_linear_probe(model, train_eval_loader, test_eval_loader)\n",
    "print(f\"Linear probe accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfd4bec",
   "metadata": {},
   "source": [
    "## Visualization: Feature Similarity\n",
    "\n",
    "Let's visualize how similar the learned features are for augmented versions of the same image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32d6487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Feature Similarity\n",
    "def visualize_feature_similarity(model, dataset, num_samples=5):\n",
    "    \"\"\"Visualize similarity between features of augmented pairs\"\"\"\n",
    "    model.eval()\n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4 * num_samples))\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            aug1, aug2 = dataset[i]\n",
    "            aug1 = aug1.unsqueeze(0).to(device)\n",
    "            aug2 = aug2.unsqueeze(0).to(device)\n",
    "            _, z1 = model(aug1)\n",
    "            _, z2 = model(aug2)\n",
    "            z1 = F.normalize(z1, dim=1)\n",
    "            z2 = F.normalize(z2, dim=1)\n",
    "            similarity = F.cosine_similarity(z1, z2).item()\n",
    "            mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "            std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "            aug1_denorm = aug1.cpu().squeeze() * std + mean\n",
    "            aug2_denorm = aug2.cpu().squeeze() * std + mean\n",
    "            axes[i, 0].imshow(aug1_denorm.permute(1, 2, 0).clamp(0, 1))\n",
    "            axes[i, 0].set_title(f'Augmentation 1')\n",
    "            axes[i, 0].axis('off')\n",
    "            axes[i, 1].imshow(aug2_denorm.permute(1, 2, 0).clamp(0, 1))\n",
    "            axes[i, 1].set_title(f'Augmentation 2')\n",
    "            axes[i, 1].axis('off')\n",
    "            axes[i, 2].text(0.5, 0.5, f'Cosine Similarity:\\n{similarity:.3f}',\n",
    "                            ha='center', va='center', fontsize=16,\n",
    "                            transform=axes[i, 2].transAxes)\n",
    "            axes[i, 2].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize feature similarity\n",
    "print(\"Visualizing feature similarity for augmented pairs\")\n",
    "visualize_feature_similarity(model, simclr_dataset, num_samples=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2f8b1f",
   "metadata": {},
   "source": [
    "# Key Insights and Best Practices\n",
    "\n",
    "### Important Findings from SimCLR Research\n",
    "\n",
    "1. **Data Augmentation is Critical**: The choice of augmentation significantly impacts performance.\n",
    "2. **Projection Head Matters**: Using a non-linear projection head during training (but not during evaluation) improves performance.\n",
    "3. **Large Batch Sizes**: Larger batch sizes provide more negative examples and generally lead to better performance.\n",
    "4. **Temperature Parameter**: The temperature in the NT-Xent loss needs to be tuned carefully (typically around 0.07-0.1).\n",
    "5. **Training Duration**: SimCLR typically requires longer training than supervised learning.\n",
    "\n",
    "### Advantages of SimCLR\n",
    "\n",
    "- **No manual labeling required**: Learns from unlabeled data.\n",
    "- **Generalizable representations**: Features transfer well to downstream tasks.\n",
    "- **Scalable**: Can leverage large amounts of unlabeled data.\n",
    "- **Simple framework**: Relatively straightforward to implement and understand.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- **Computational requirements**: Requires large batch sizes and long training times.\n",
    "- **Memory intensive**: Storing features for all samples in a batch can be memory-intensive.\n",
    "- **Hyperparameter sensitivity**: Performance can be sensitive to augmentation choices and temperature.\n",
    "\n",
    "### Applications\n",
    "\n",
    "- **Image classification**: Pre-training for downstream classification tasks.\n",
    "- **Object detection**: Learning robust visual features for detection models.\n",
    "- **Medical imaging**: Learning representations from unlabeled medical images.\n",
    "- **Remote sensing**: Analyzing satellite imagery without manual annotations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
