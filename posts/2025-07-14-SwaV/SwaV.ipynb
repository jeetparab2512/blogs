{
 "cells": [
  {
   "cell_type": "raw",
   "id": "1033d2d5",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"SwaV: Swapping Assignments between Views for Unsupervised Learning of Visual Features\"\n",
    "date: \"2025-07-12\"\n",
    "categories: [self-supervised, contrastive-learning, vision]\n",
    "description: \"Exploring Swapping Assignments between Views for Unsupervised Learning of Visual Features\"\n",
    "output-file: 2025-07-12-swav.html\n",
    "badges: true\n",
    "toc: true\n",
    "author: Jeet Parab\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e449a5e8",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "SwAV (Swapping Assignments between Views) is a self-supervised learning method for visual representation learning introduced by Caron et al. in 2020.  \n",
    "Unlike contrastive methods that rely on negative sampling, SwAV adopts a **clustering-based approach**, encouraging consistency between cluster assignments from different augmentations of the same image.\n",
    "\n",
    "::: {.callout-note}\n",
    "**Read the original paper:**  **Caron, Mathilde, et al.**  \n",
    "*Unsupervised Learning of Visual Features by Contrasting Cluster Assignments (SwaV)* (2020)  \n",
    "[arXiv:2006.09882](https://arxiv.org/abs/2006.09882)\n",
    ":::\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### 1. Multi-Crop Strategy\n",
    "\n",
    "SwAV introduces a **multi-crop augmentation** technique to expose the model to both global context and local details.\n",
    "\n",
    "- **Global crops**: Two high-resolution crops of size 224×224  \n",
    "- **Local crops**: Several smaller crops, typically of size 96×96  \n",
    "\n",
    "This strategy increases data diversity without increasing batch size.\n",
    "\n",
    "### 2. Clustering-Based Learning\n",
    "\n",
    "Instead of comparing positive and negative pairs, SwAV:\n",
    "\n",
    "- Maps input images to **feature embeddings** using a backbone network  \n",
    "- Assigns these embeddings to a set of **learnable prototypes** (i.e., cluster centers)  \n",
    "- Enforces **consistency** between the prototype assignments of different views (augmentations) of the same image  \n",
    "\n",
    "This avoids the need for explicit negative samples while encouraging invariant representations.\n",
    "\n",
    "### 3. Sinkhorn-Knopp Algorithm\n",
    "\n",
    "SwAV uses the **Sinkhorn-Knopp algorithm** to obtain **balanced assignments** to clusters.  \n",
    "\n",
    "This step solves an **optimal transport** problem, ensuring that each prototype receives approximately equal assignment probability, which helps prevent collapse (i.e., all embeddings mapping to the same cluster).\n",
    "\n",
    "The algorithm normalizes the assignment matrix iteratively so that:\n",
    "\n",
    "- Each row sums to 1 (each image maps to a probability distribution over prototypes)  \n",
    "- Each column sums to 1 (each prototype is used evenly across the batch)  \n",
    "\n",
    "This balanced soft-clustering technique is key to SwAV's success without requiring contrastive loss.\n",
    "\n",
    "![SwaV Architecture](/assets/SwaV.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db704fab",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde19f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "## SwAV Model Architecture\n",
    "\n",
    "class SwAVModel(nn.Module):\n",
    "    \"\"\"\n",
    "    SwAV model with ResNet-like backbone for CIFAR-10\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone_dim=512, num_prototypes=1000, projection_dim=128):\n",
    "        super(SwAVModel, self).__init__()\n",
    "        \n",
    "        # CIFAR-10 optimized backbone\n",
    "        self.backbone = nn.Sequential(\n",
    "            # First conv block\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Second conv block\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Third conv block\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Fourth conv block\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Global average pooling\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        # Projection head\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(512, backbone_dim),\n",
    "            nn.BatchNorm1d(backbone_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(backbone_dim, projection_dim)\n",
    "        )\n",
    "        \n",
    "        # Prototypes (learnable cluster centers)\n",
    "        self.prototypes = nn.Linear(projection_dim, num_prototypes, bias=False)\n",
    "        \n",
    "        # Initialize prototypes\n",
    "        self.prototypes.weight.data.normal_(0, 0.01)\n",
    "        self.prototypes.weight.data = F.normalize(self.prototypes.weight.data, dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Extract features\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        # Project features\n",
    "        z = self.projection_head(features)\n",
    "        z = F.normalize(z, dim=1)\n",
    "        \n",
    "        # Compute prototype scores\n",
    "        scores = self.prototypes(z)\n",
    "        \n",
    "        return z, scores\n",
    "\n",
    "# Test model\n",
    "model = SwAVModel()\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.randn(2, 3, 224, 224)\n",
    "z, scores = model(test_input)\n",
    "print(f\"Feature shape: {z.shape}\")\n",
    "print(f\"Scores shape: {scores.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c330337",
   "metadata": {},
   "source": [
    "## Sinkhorn-Knopp Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf797ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinkhorn_knopp(Q, num_iters=3, epsilon=0.05):\n",
    "    \"\"\"\n",
    "    Sinkhorn-Knopp algorithm for optimal transport\n",
    "    \n",
    "    Args:\n",
    "        Q: Matrix of prototype scores [batch_size, num_prototypes]\n",
    "        num_iters: Number of iterations\n",
    "        epsilon: Temperature parameter\n",
    "    \n",
    "    Returns:\n",
    "        Normalized assignment matrix\n",
    "    \"\"\"\n",
    "    Q = torch.exp(Q / epsilon)\n",
    "    B, K = Q.shape\n",
    "    \n",
    "    # Make the matrix doubly stochastic\n",
    "    for _ in range(num_iters):\n",
    "        # Normalize rows (sum to 1 across prototypes)\n",
    "        Q = Q / (Q.sum(dim=1, keepdim=True) + 1e-8)\n",
    "        # Normalize columns (balanced assignments)\n",
    "        Q = Q / (Q.sum(dim=0, keepdim=True) + 1e-8)\n",
    "        # Rescale\n",
    "        Q = Q * B\n",
    "    \n",
    "    return Q\n",
    "\n",
    "# Test Sinkhorn-Knopp\n",
    "test_scores = torch.randn(4, 10)\n",
    "assignments = sinkhorn_knopp(test_scores)\n",
    "print(f\"Assignment matrix shape: {assignments.shape}\")\n",
    "print(f\"Row sums: {assignments.sum(dim=1)}\")\n",
    "print(f\"Column sums: {assignments.sum(dim=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e8a947",
   "metadata": {},
   "source": [
    "## SwAV Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce2a606",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwAVLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    SwAV loss function implementing the swapped prediction objective\n",
    "    \"\"\"\n",
    "    def __init__(self, temperature=0.1, epsilon=0.05, sinkhorn_iterations=3):\n",
    "        super(SwAVLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.epsilon = epsilon\n",
    "        self.sinkhorn_iterations = sinkhorn_iterations\n",
    "    \n",
    "    def forward(self, z_list, scores_list):\n",
    "        \"\"\"\n",
    "        Compute SwAV loss for multiple views\n",
    "        \n",
    "        Args:\n",
    "            z_list: List of feature tensors from different views\n",
    "            scores_list: List of prototype scores from different views\n",
    "        \"\"\"\n",
    "        total_loss = 0\n",
    "        num_views = len(z_list)\n",
    "        \n",
    "        for i in range(num_views):\n",
    "            for j in range(num_views):\n",
    "                if i != j:\n",
    "                    # Get assignments from view i\n",
    "                    with torch.no_grad():\n",
    "                        q_i = sinkhorn_knopp(\n",
    "                            scores_list[i], \n",
    "                            self.sinkhorn_iterations, \n",
    "                            self.epsilon\n",
    "                        )\n",
    "                    \n",
    "                    # Get predictions from view j\n",
    "                    p_j = F.softmax(scores_list[j] / self.temperature, dim=1)\n",
    "                    \n",
    "                    # Cross-entropy loss\n",
    "                    loss = -torch.mean(torch.sum(q_i * torch.log(p_j + 1e-8), dim=1))\n",
    "                    total_loss += loss\n",
    "        \n",
    "        return total_loss / (num_views * (num_views - 1))\n",
    "\n",
    "# Test loss function\n",
    "loss_fn = SwAVLoss()\n",
    "test_z = [torch.randn(4, 128) for _ in range(4)]\n",
    "test_scores = [torch.randn(4, 10) for _ in range(4)]\n",
    "test_loss = loss_fn(test_z, test_scores)\n",
    "print(f\"Test loss: {test_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d965c27",
   "metadata": {},
   "source": [
    "## CIFAR-10 Multi-Crop Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d6a440",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10MultiCrop(Dataset):\n",
    "    \"\"\"\n",
    "    CIFAR-10 dataset with multi-crop augmentation for SwAV\n",
    "    \"\"\"\n",
    "    def __init__(self, train=True, download=True, \n",
    "                 global_crop_size=224, local_crop_size=96, num_local_crops=6):\n",
    "        # Load CIFAR-10 dataset\n",
    "        self.cifar10 = torchvision.datasets.CIFAR10(\n",
    "            root='./data', \n",
    "            train=train, \n",
    "            download=download,\n",
    "            transform=None\n",
    "        )\n",
    "        \n",
    "        # Global crop transforms (high resolution)\n",
    "        self.global_transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(global_crop_size, scale=(0.4, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "        ])\n",
    "        \n",
    "        # Local crop transforms (lower resolution)\n",
    "        self.local_transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(local_crop_size, scale=(0.05, 0.4)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "        ])\n",
    "        \n",
    "        self.num_local_crops = num_local_crops\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.cifar10)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image, _ = self.cifar10[idx]  # Ignore labels for self-supervised learning\n",
    "        \n",
    "        # Generate 2 global crops\n",
    "        global_crops = [self.global_transform(image) for _ in range(2)]\n",
    "        \n",
    "        # Generate multiple local crops\n",
    "        local_crops = [self.local_transform(image) for _ in range(self.num_local_crops)]\n",
    "        \n",
    "        return global_crops + local_crops\n",
    "\n",
    "# Create dataset and dataloader\n",
    "train_dataset = CIFAR10MultiCrop(train=True, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "print(f\"Dataset size: {len(train_dataset)}\")\n",
    "print(f\"Number of batches: {len(train_loader)}\")\n",
    "\n",
    "# Visualize some crops\n",
    "sample_crops = train_dataset[0]\n",
    "print(f\"Number of crops per image: {len(sample_crops)}\")\n",
    "print(f\"Global crop 1 shape: {sample_crops[0].shape}\")\n",
    "print(f\"Local crop 1 shape: {sample_crops[2].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8247bb6b",
   "metadata": {},
   "source": [
    "## Visualization of Multi-Crop Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20860cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_multicrop_sample():\n",
    "    \"\"\"Visualize the multi-crop strategy on a CIFAR-10 sample\"\"\"\n",
    "    # Get original CIFAR-10 image\n",
    "    cifar10_orig = torchvision.datasets.CIFAR10(root='./data', train=True, download=False)\n",
    "    orig_image, label = cifar10_orig[100]\n",
    "    \n",
    "    # Get multi-crop version\n",
    "    crops = train_dataset[100]\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0, 0].imshow(orig_image)\n",
    "    axes[0, 0].set_title('Original\\nCIFAR-10')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    # Global crops\n",
    "    for i in range(2):\n",
    "        crop = crops[i]\n",
    "        # Denormalize for visualization\n",
    "        crop = crop * torch.tensor([0.2023, 0.1994, 0.2010]).view(3, 1, 1)\n",
    "        crop = crop + torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n",
    "        crop = torch.clamp(crop, 0, 1)\n",
    "        \n",
    "        axes[0, i+1].imshow(crop.permute(1, 2, 0))\n",
    "        axes[0, i+1].set_title(f'Global Crop {i+1}\\n224×224')\n",
    "        axes[0, i+1].axis('off')\n",
    "    \n",
    "    # Local crops (first 6)\n",
    "    for i in range(6):\n",
    "        crop = crops[i+2]\n",
    "        # Denormalize for visualization\n",
    "        crop = crop * torch.tensor([0.2023, 0.1994, 0.2010]).view(3, 1, 1)\n",
    "        crop = crop + torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n",
    "        crop = torch.clamp(crop, 0, 1)\n",
    "        \n",
    "        row = 0 if i < 3 else 1\n",
    "        col = (i % 3) + 2\n",
    "        if row == 1:\n",
    "            col = (i % 3)\n",
    "        \n",
    "        axes[row, col].imshow(crop.permute(1, 2, 0))\n",
    "        axes[row, col].set_title(f'Local Crop {i+1}\\n96×96')\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(3, 5):\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_multicrop_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f0f045",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e823a389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_swav(model, train_loader, num_epochs=10, lr=0.001):\n",
    "    \"\"\"\n",
    "    Train SwAV model on CIFAR-10\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    criterion = SwAVLoss()\n",
    "    \n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    print(f\"Training SwAV on CIFAR-10 for {num_epochs} epochs\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_idx, crops in enumerate(train_loader):\n",
    "            try:\n",
    "                # Move crops to device\n",
    "                crops = [crop.to(device) for crop in crops]\n",
    "                \n",
    "                # Forward pass through all crops\n",
    "                z_list = []\n",
    "                scores_list = []\n",
    "                \n",
    "                for crop in crops:\n",
    "                    z, scores = model(crop)\n",
    "                    z_list.append(z)\n",
    "                    scores_list.append(scores)\n",
    "                \n",
    "                # Compute SwAV loss\n",
    "                loss = criterion(z_list, scores_list)\n",
    "                \n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Normalize prototypes\n",
    "                with torch.no_grad():\n",
    "                    model.prototypes.weight.data = F.normalize(\n",
    "                        model.prototypes.weight.data, dim=1\n",
    "                    )\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                \n",
    "                if batch_idx % 50 == 0:\n",
    "                    print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {batch_idx}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if num_batches > 0:\n",
    "            avg_loss = epoch_loss / num_batches\n",
    "            losses.append(avg_loss)\n",
    "            print(f'Epoch {epoch+1} Complete - Avg Loss: {avg_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}')\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Initialize model and start training\n",
    "model = SwAVModel(backbone_dim=512, num_prototypes=500, projection_dim=128)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Train the model\n",
    "losses = train_swav(model, train_loader, num_epochs=5, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e014fae5",
   "metadata": {},
   "source": [
    "## Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01932689",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses, 'b-', linewidth=2, marker='o')\n",
    "plt.title('SwAV Training Loss on CIFAR-10')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Extract features for visualization\n",
    "model.eval()\n",
    "feature_extractor = nn.Sequential(model.backbone, model.projection_head)\n",
    "\n",
    "# Simple dataset for feature extraction\n",
    "simple_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    ")\n",
    "\n",
    "simple_loader = DataLoader(simple_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "# Extract features\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, batch_labels) in enumerate(simple_loader):\n",
    "        if batch_idx >= 10:  # Limit to first 1000 samples\n",
    "            break\n",
    "        images = images.to(device)\n",
    "        batch_features = feature_extractor(images)\n",
    "        features.append(batch_features.cpu())\n",
    "        labels.append(batch_labels)\n",
    "\n",
    "features = torch.cat(features, dim=0).numpy()\n",
    "labels = torch.cat(labels, dim=0).numpy()\n",
    "\n",
    "# PCA visualization\n",
    "pca = PCA(n_components=2)\n",
    "features_2d = pca.fit_transform(features)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
    "\n",
    "for i, class_name in enumerate(classes):\n",
    "    mask = labels == i\n",
    "    plt.scatter(features_2d[mask, 0], features_2d[mask, 1], \n",
    "               c=[colors[i]], label=class_name, alpha=0.6, s=20)\n",
    "\n",
    "plt.title('SwAV Features PCA Visualization')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Feature extraction completed: {features.shape[0]} samples, {features.shape[1]} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27ba801",
   "metadata": {},
   "source": [
    "## Feature Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2df7080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature quality\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Feature distribution\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(features.flatten(), bins=50, alpha=0.7, color='skyblue')\n",
    "plt.title('Feature Value Distribution')\n",
    "plt.xlabel('Feature Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Feature variance across dimensions\n",
    "plt.subplot(1, 3, 2)\n",
    "feature_var = np.var(features, axis=0)\n",
    "plt.plot(feature_var, 'g-', linewidth=2)\n",
    "plt.title('Feature Variance per Dimension')\n",
    "plt.xlabel('Feature Dimension')\n",
    "plt.ylabel('Variance')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Feature correlation matrix (subset)\n",
    "plt.subplot(1, 3, 3)\n",
    "correlation_matrix = np.corrcoef(features[:, :32].T)  # First 32 dimensions\n",
    "plt.imshow(correlation_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Feature Correlation Matrix\\n(First 32 dimensions)')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Feature Statistics:\")\n",
    "print(f\"Mean: {features.mean():.4f}\")\n",
    "print(f\"Std: {features.std():.4f}\")\n",
    "print(f\"Min: {features.min():.4f}\")\n",
    "print(f\"Max: {features.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6fd1b9",
   "metadata": {},
   "source": [
    "## Key Advantages of SwAV\n",
    "\n",
    "1. **No Negative Sampling**: Unlike contrastive methods, SwAV doesn't require negative pairs\n",
    "2. **Scalability**: Works well with large batch sizes and many prototypes\n",
    "3. **Multi-scale Learning**: Uses crops of different sizes for better representation learning\n",
    "4. **Balanced Assignments**: Sinkhorn-Knopp ensures balanced cluster assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2053fe",
   "metadata": {},
   "source": [
    "## Comparison with Other Methods\n",
    "\n",
    "| Method | Approach | Key Innovation |\n",
    "|--------|----------|----------------|\n",
    "| SimCLR | Contrastive | Large batch sizes + strong augmentation |\n",
    "| MoCo | Contrastive | Momentum encoder + queue |\n",
    "| SwAV | Clustering | Prototype-based assignments + multi-crop |\n",
    "| BYOL | Non-contrastive | Predictor network + stop gradient |\n",
    "\n",
    "## Practical Considerations\n",
    "\n",
    "1. **Prototype Initialization**: Prototypes should be normalized and well-initialized\n",
    "2. **Sinkhorn Iterations**: Usually 3 iterations are sufficient\n",
    "3. **Temperature Scaling**: Important for balancing assignments\n",
    "4. **Multi-crop Ratios**: Typically 2 global + 6 local crops\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d6ac05",
   "metadata": {},
   "source": [
    "## Future Directions\n",
    "\n",
    "SwAV has inspired several important follow-up works in self-supervised learning:\n",
    "\n",
    "- **SeLa**: Integrates SwAV-style clustering with momentum encoder updates for improved stability.\n",
    "\n",
    "- **DenseCL**: Adapts SwAV principles for dense prediction tasks such as object detection and segmentation.\n",
    "\n",
    "- **SwAV+**: Enhances the original SwAV with stronger augmentations and improved architectural choices.\n",
    "\n",
    "This implementation serves as a strong foundation for understanding and experimenting with SwAV.\n",
    "\n",
    "For real-world or production-level applications, consider using the **official implementation**, which includes robust ResNet backbones, better training schedules, and optimized performance settings.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
