[
  {
    "objectID": "posts/2025-07-14-SwaV/2025-07-12-swav.html",
    "href": "posts/2025-07-14-SwaV/2025-07-12-swav.html",
    "title": "SwaV: Swapping Assignments between Views for Unsupervised Learning of Visual Features",
    "section": "",
    "text": "SwAV (Swapping Assignments between Views) is a self-supervised learning method for visual representation learning introduced by Caron et al. in 2020.\nUnlike contrastive methods that rely on negative sampling, SwAV adopts a clustering-based approach, encouraging consistency between cluster assignments from different augmentations of the same image.\n\n\n\n\n\n\nNote\n\n\n\nRead the original paper: Caron, Mathilde, et al.\nUnsupervised Learning of Visual Features by Contrasting Cluster Assignments (SwaV) (2020)\narXiv:2006.09882"
  },
  {
    "objectID": "posts/2025-07-14-SwaV/2025-07-12-swav.html#introduction",
    "href": "posts/2025-07-14-SwaV/2025-07-12-swav.html#introduction",
    "title": "SwaV: Swapping Assignments between Views for Unsupervised Learning of Visual Features",
    "section": "",
    "text": "SwAV (Swapping Assignments between Views) is a self-supervised learning method for visual representation learning introduced by Caron et al. in 2020.\nUnlike contrastive methods that rely on negative sampling, SwAV adopts a clustering-based approach, encouraging consistency between cluster assignments from different augmentations of the same image.\n\n\n\n\n\n\nNote\n\n\n\nRead the original paper: Caron, Mathilde, et al.\nUnsupervised Learning of Visual Features by Contrasting Cluster Assignments (SwaV) (2020)\narXiv:2006.09882"
  },
  {
    "objectID": "posts/2025-07-14-SwaV/2025-07-12-swav.html#key-concepts",
    "href": "posts/2025-07-14-SwaV/2025-07-12-swav.html#key-concepts",
    "title": "SwaV: Swapping Assignments between Views for Unsupervised Learning of Visual Features",
    "section": "Key Concepts",
    "text": "Key Concepts\n\n1. Multi-Crop Strategy\nSwAV introduces a multi-crop augmentation technique to expose the model to both global context and local details.\n\nGlobal crops: Two high-resolution crops of size 224×224\n\nLocal crops: Several smaller crops, typically of size 96×96\n\nThis strategy increases data diversity without increasing batch size.\n\n\n2. Clustering-Based Learning\nInstead of comparing positive and negative pairs, SwAV:\n\nMaps input images to feature embeddings using a backbone network\n\nAssigns these embeddings to a set of learnable prototypes (i.e., cluster centers)\n\nEnforces consistency between the prototype assignments of different views (augmentations) of the same image\n\nThis avoids the need for explicit negative samples while encouraging invariant representations.\n\n\n3. Sinkhorn-Knopp Algorithm\nSwAV uses the Sinkhorn-Knopp algorithm to obtain balanced assignments to clusters.\nThis step solves an optimal transport problem, ensuring that each prototype receives approximately equal assignment probability, which helps prevent collapse (i.e., all embeddings mapping to the same cluster).\nThe algorithm normalizes the assignment matrix iteratively so that:\n\nEach row sums to 1 (each image maps to a probability distribution over prototypes)\n\nEach column sums to 1 (each prototype is used evenly across the batch)\n\nThis balanced soft-clustering technique is key to SwAV’s success without requiring contrastive loss.\n\n\n\nSwaV Architecture"
  },
  {
    "objectID": "posts/2025-07-14-SwaV/2025-07-12-swav.html#implementation",
    "href": "posts/2025-07-14-SwaV/2025-07-12-swav.html#implementation",
    "title": "SwaV: Swapping Assignments between Views for Unsupervised Learning of Visual Features",
    "section": "Implementation",
    "text": "Implementation\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport numpy as np\nfrom torch.utils.data import DataLoader, Dataset\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport random\nfrom sklearn.decomposition import PCA\nimport os\n\n# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\n\n# Check device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n## SwAV Model Architecture\n\nclass SwAVModel(nn.Module):\n    \"\"\"\n    SwAV model with ResNet-like backbone for CIFAR-10\n    \"\"\"\n    def __init__(self, backbone_dim=512, num_prototypes=1000, projection_dim=128):\n        super(SwAVModel, self).__init__()\n        \n        # CIFAR-10 optimized backbone\n        self.backbone = nn.Sequential(\n            # First conv block\n            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            \n            # Second conv block\n            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            \n            # Third conv block\n            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            \n            # Fourth conv block\n            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            \n            # Global average pooling\n            nn.AdaptiveAvgPool2d((1, 1)),\n            nn.Flatten()\n        )\n        \n        # Projection head\n        self.projection_head = nn.Sequential(\n            nn.Linear(512, backbone_dim),\n            nn.BatchNorm1d(backbone_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(backbone_dim, projection_dim)\n        )\n        \n        # Prototypes (learnable cluster centers)\n        self.prototypes = nn.Linear(projection_dim, num_prototypes, bias=False)\n        \n        # Initialize prototypes\n        self.prototypes.weight.data.normal_(0, 0.01)\n        self.prototypes.weight.data = F.normalize(self.prototypes.weight.data, dim=1)\n        \n    def forward(self, x):\n        # Extract features\n        features = self.backbone(x)\n        \n        # Project features\n        z = self.projection_head(features)\n        z = F.normalize(z, dim=1)\n        \n        # Compute prototype scores\n        scores = self.prototypes(z)\n        \n        return z, scores\n\n# Test model\nmodel = SwAVModel()\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n# Test forward pass\ntest_input = torch.randn(2, 3, 224, 224)\nz, scores = model(test_input)\nprint(f\"Feature shape: {z.shape}\")\nprint(f\"Scores shape: {scores.shape}\")"
  },
  {
    "objectID": "posts/2025-07-14-SwaV/2025-07-12-swav.html#sinkhorn-knopp-algorithm-1",
    "href": "posts/2025-07-14-SwaV/2025-07-12-swav.html#sinkhorn-knopp-algorithm-1",
    "title": "SwaV: Swapping Assignments between Views for Unsupervised Learning of Visual Features",
    "section": "Sinkhorn-Knopp Algorithm",
    "text": "Sinkhorn-Knopp Algorithm\n\ndef sinkhorn_knopp(Q, num_iters=3, epsilon=0.05):\n    \"\"\"\n    Sinkhorn-Knopp algorithm for optimal transport\n    \n    Args:\n        Q: Matrix of prototype scores [batch_size, num_prototypes]\n        num_iters: Number of iterations\n        epsilon: Temperature parameter\n    \n    Returns:\n        Normalized assignment matrix\n    \"\"\"\n    Q = torch.exp(Q / epsilon)\n    B, K = Q.shape\n    \n    # Make the matrix doubly stochastic\n    for _ in range(num_iters):\n        # Normalize rows (sum to 1 across prototypes)\n        Q = Q / (Q.sum(dim=1, keepdim=True) + 1e-8)\n        # Normalize columns (balanced assignments)\n        Q = Q / (Q.sum(dim=0, keepdim=True) + 1e-8)\n        # Rescale\n        Q = Q * B\n    \n    return Q\n\n# Test Sinkhorn-Knopp\ntest_scores = torch.randn(4, 10)\nassignments = sinkhorn_knopp(test_scores)\nprint(f\"Assignment matrix shape: {assignments.shape}\")\nprint(f\"Row sums: {assignments.sum(dim=1)}\")\nprint(f\"Column sums: {assignments.sum(dim=0)}\")"
  },
  {
    "objectID": "posts/2025-07-14-SwaV/2025-07-12-swav.html#swav-loss-function",
    "href": "posts/2025-07-14-SwaV/2025-07-12-swav.html#swav-loss-function",
    "title": "SwaV: Swapping Assignments between Views for Unsupervised Learning of Visual Features",
    "section": "SwAV Loss Function",
    "text": "SwAV Loss Function\n\nclass SwAVLoss(nn.Module):\n    \"\"\"\n    SwAV loss function implementing the swapped prediction objective\n    \"\"\"\n    def __init__(self, temperature=0.1, epsilon=0.05, sinkhorn_iterations=3):\n        super(SwAVLoss, self).__init__()\n        self.temperature = temperature\n        self.epsilon = epsilon\n        self.sinkhorn_iterations = sinkhorn_iterations\n    \n    def forward(self, z_list, scores_list):\n        \"\"\"\n        Compute SwAV loss for multiple views\n        \n        Args:\n            z_list: List of feature tensors from different views\n            scores_list: List of prototype scores from different views\n        \"\"\"\n        total_loss = 0\n        num_views = len(z_list)\n        \n        for i in range(num_views):\n            for j in range(num_views):\n                if i != j:\n                    # Get assignments from view i\n                    with torch.no_grad():\n                        q_i = sinkhorn_knopp(\n                            scores_list[i], \n                            self.sinkhorn_iterations, \n                            self.epsilon\n                        )\n                    \n                    # Get predictions from view j\n                    p_j = F.softmax(scores_list[j] / self.temperature, dim=1)\n                    \n                    # Cross-entropy loss\n                    loss = -torch.mean(torch.sum(q_i * torch.log(p_j + 1e-8), dim=1))\n                    total_loss += loss\n        \n        return total_loss / (num_views * (num_views - 1))\n\n# Test loss function\nloss_fn = SwAVLoss()\ntest_z = [torch.randn(4, 128) for _ in range(4)]\ntest_scores = [torch.randn(4, 10) for _ in range(4)]\ntest_loss = loss_fn(test_z, test_scores)\nprint(f\"Test loss: {test_loss.item():.4f}\")"
  },
  {
    "objectID": "posts/2025-07-14-SwaV/2025-07-12-swav.html#cifar-10-multi-crop-dataset",
    "href": "posts/2025-07-14-SwaV/2025-07-12-swav.html#cifar-10-multi-crop-dataset",
    "title": "SwaV: Swapping Assignments between Views for Unsupervised Learning of Visual Features",
    "section": "CIFAR-10 Multi-Crop Dataset",
    "text": "CIFAR-10 Multi-Crop Dataset\n\nclass CIFAR10MultiCrop(Dataset):\n    \"\"\"\n    CIFAR-10 dataset with multi-crop augmentation for SwAV\n    \"\"\"\n    def __init__(self, train=True, download=True, \n                 global_crop_size=224, local_crop_size=96, num_local_crops=6):\n        # Load CIFAR-10 dataset\n        self.cifar10 = torchvision.datasets.CIFAR10(\n            root='./data', \n            train=train, \n            download=download,\n            transform=None\n        )\n        \n        # Global crop transforms (high resolution)\n        self.global_transform = transforms.Compose([\n            transforms.RandomResizedCrop(global_crop_size, scale=(0.4, 1.0)),\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n            transforms.RandomGrayscale(p=0.2),\n            transforms.ToTensor(),\n            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n        ])\n        \n        # Local crop transforms (lower resolution)\n        self.local_transform = transforms.Compose([\n            transforms.RandomResizedCrop(local_crop_size, scale=(0.05, 0.4)),\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n            transforms.RandomGrayscale(p=0.2),\n            transforms.ToTensor(),\n            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n        ])\n        \n        self.num_local_crops = num_local_crops\n        \n    def __len__(self):\n        return len(self.cifar10)\n    \n    def __getitem__(self, idx):\n        image, _ = self.cifar10[idx]  # Ignore labels for self-supervised learning\n        \n        # Generate 2 global crops\n        global_crops = [self.global_transform(image) for _ in range(2)]\n        \n        # Generate multiple local crops\n        local_crops = [self.local_transform(image) for _ in range(self.num_local_crops)]\n        \n        return global_crops + local_crops\n\n# Create dataset and dataloader\ntrain_dataset = CIFAR10MultiCrop(train=True, download=True)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n\nprint(f\"Dataset size: {len(train_dataset)}\")\nprint(f\"Number of batches: {len(train_loader)}\")\n\n# Visualize some crops\nsample_crops = train_dataset[0]\nprint(f\"Number of crops per image: {len(sample_crops)}\")\nprint(f\"Global crop 1 shape: {sample_crops[0].shape}\")\nprint(f\"Local crop 1 shape: {sample_crops[2].shape}\")"
  },
  {
    "objectID": "posts/2025-07-14-SwaV/2025-07-12-swav.html#visualization-of-multi-crop-strategy",
    "href": "posts/2025-07-14-SwaV/2025-07-12-swav.html#visualization-of-multi-crop-strategy",
    "title": "SwaV: Swapping Assignments between Views for Unsupervised Learning of Visual Features",
    "section": "Visualization of Multi-Crop Strategy",
    "text": "Visualization of Multi-Crop Strategy\n\ndef visualize_multicrop_sample():\n    \"\"\"Visualize the multi-crop strategy on a CIFAR-10 sample\"\"\"\n    # Get original CIFAR-10 image\n    cifar10_orig = torchvision.datasets.CIFAR10(root='./data', train=True, download=False)\n    orig_image, label = cifar10_orig[100]\n    \n    # Get multi-crop version\n    crops = train_dataset[100]\n    \n    # Plot\n    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n    \n    # Original image\n    axes[0, 0].imshow(orig_image)\n    axes[0, 0].set_title('Original\\nCIFAR-10')\n    axes[0, 0].axis('off')\n    \n    # Global crops\n    for i in range(2):\n        crop = crops[i]\n        # Denormalize for visualization\n        crop = crop * torch.tensor([0.2023, 0.1994, 0.2010]).view(3, 1, 1)\n        crop = crop + torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n        crop = torch.clamp(crop, 0, 1)\n        \n        axes[0, i+1].imshow(crop.permute(1, 2, 0))\n        axes[0, i+1].set_title(f'Global Crop {i+1}\\n224×224')\n        axes[0, i+1].axis('off')\n    \n    # Local crops (first 6)\n    for i in range(6):\n        crop = crops[i+2]\n        # Denormalize for visualization\n        crop = crop * torch.tensor([0.2023, 0.1994, 0.2010]).view(3, 1, 1)\n        crop = crop + torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n        crop = torch.clamp(crop, 0, 1)\n        \n        row = 0 if i &lt; 3 else 1\n        col = (i % 3) + 2\n        if row == 1:\n            col = (i % 3)\n        \n        axes[row, col].imshow(crop.permute(1, 2, 0))\n        axes[row, col].set_title(f'Local Crop {i+1}\\n96×96')\n        axes[row, col].axis('off')\n    \n    # Hide unused subplots\n    for i in range(3, 5):\n        axes[1, i].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\nvisualize_multicrop_sample()"
  },
  {
    "objectID": "posts/2025-07-14-SwaV/2025-07-12-swav.html#training-function",
    "href": "posts/2025-07-14-SwaV/2025-07-12-swav.html#training-function",
    "title": "SwaV: Swapping Assignments between Views for Unsupervised Learning of Visual Features",
    "section": "Training Function",
    "text": "Training Function\n\ndef train_swav(model, train_loader, num_epochs=10, lr=0.001):\n    \"\"\"\n    Train SwAV model on CIFAR-10\n    \"\"\"\n    model.to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n    criterion = SwAVLoss()\n    \n    model.train()\n    losses = []\n    \n    print(f\"Training SwAV on CIFAR-10 for {num_epochs} epochs\")\n    print(f\"Device: {device}\")\n    print(\"-\" * 60)\n    \n    for epoch in range(num_epochs):\n        epoch_loss = 0\n        num_batches = 0\n        \n        for batch_idx, crops in enumerate(train_loader):\n            try:\n                # Move crops to device\n                crops = [crop.to(device) for crop in crops]\n                \n                # Forward pass through all crops\n                z_list = []\n                scores_list = []\n                \n                for crop in crops:\n                    z, scores = model(crop)\n                    z_list.append(z)\n                    scores_list.append(scores)\n                \n                # Compute SwAV loss\n                loss = criterion(z_list, scores_list)\n                \n                # Backward pass\n                optimizer.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                optimizer.step()\n                \n                # Normalize prototypes\n                with torch.no_grad():\n                    model.prototypes.weight.data = F.normalize(\n                        model.prototypes.weight.data, dim=1\n                    )\n                \n                epoch_loss += loss.item()\n                num_batches += 1\n                \n                if batch_idx % 50 == 0:\n                    print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n                \n            except Exception as e:\n                print(f\"Error in batch {batch_idx}: {e}\")\n                continue\n        \n        scheduler.step()\n        \n        if num_batches &gt; 0:\n            avg_loss = epoch_loss / num_batches\n            losses.append(avg_loss)\n            print(f'Epoch {epoch+1} Complete - Avg Loss: {avg_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}')\n        \n        print(\"-\" * 60)\n    \n    return losses\n\n# Initialize model and start training\nmodel = SwAVModel(backbone_dim=512, num_prototypes=500, projection_dim=128)\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n# Train the model\nlosses = train_swav(model, train_loader, num_epochs=5, lr=0.001)"
  },
  {
    "objectID": "posts/2025-07-14-SwaV/2025-07-12-swav.html#results-visualization",
    "href": "posts/2025-07-14-SwaV/2025-07-12-swav.html#results-visualization",
    "title": "SwaV: Swapping Assignments between Views for Unsupervised Learning of Visual Features",
    "section": "Results Visualization",
    "text": "Results Visualization\n\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(losses, 'b-', linewidth=2, marker='o')\nplt.title('SwAV Training Loss on CIFAR-10')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.grid(True, alpha=0.3)\n\n# Extract features for visualization\nmodel.eval()\nfeature_extractor = nn.Sequential(model.backbone, model.projection_head)\n\n# Simple dataset for feature extraction\nsimple_dataset = torchvision.datasets.CIFAR10(\n    root='./data', train=False, download=True,\n    transform=transforms.Compose([\n        transforms.Resize(224),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n    ])\n)\n\nsimple_loader = DataLoader(simple_dataset, batch_size=100, shuffle=False)\n\n# Extract features\nfeatures = []\nlabels = []\n\nwith torch.no_grad():\n    for batch_idx, (images, batch_labels) in enumerate(simple_loader):\n        if batch_idx &gt;= 10:  # Limit to first 1000 samples\n            break\n        images = images.to(device)\n        batch_features = feature_extractor(images)\n        features.append(batch_features.cpu())\n        labels.append(batch_labels)\n\nfeatures = torch.cat(features, dim=0).numpy()\nlabels = torch.cat(labels, dim=0).numpy()\n\n# PCA visualization\npca = PCA(n_components=2)\nfeatures_2d = pca.fit_transform(features)\n\nplt.subplot(1, 2, 2)\nclasses = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\ncolors = plt.cm.tab10(np.linspace(0, 1, 10))\n\nfor i, class_name in enumerate(classes):\n    mask = labels == i\n    plt.scatter(features_2d[mask, 0], features_2d[mask, 1], \n               c=[colors[i]], label=class_name, alpha=0.6, s=20)\n\nplt.title('SwAV Features PCA Visualization')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Feature extraction completed: {features.shape[0]} samples, {features.shape[1]} dimensions\")"
  },
  {
    "objectID": "posts/2025-07-14-SwaV/2025-07-12-swav.html#feature-quality-analysis",
    "href": "posts/2025-07-14-SwaV/2025-07-12-swav.html#feature-quality-analysis",
    "title": "SwaV: Swapping Assignments between Views for Unsupervised Learning of Visual Features",
    "section": "Feature Quality Analysis",
    "text": "Feature Quality Analysis\n\n# Analyze feature quality\nplt.figure(figsize=(15, 5))\n\n# Feature distribution\nplt.subplot(1, 3, 1)\nplt.hist(features.flatten(), bins=50, alpha=0.7, color='skyblue')\nplt.title('Feature Value Distribution')\nplt.xlabel('Feature Value')\nplt.ylabel('Frequency')\nplt.grid(True, alpha=0.3)\n\n# Feature variance across dimensions\nplt.subplot(1, 3, 2)\nfeature_var = np.var(features, axis=0)\nplt.plot(feature_var, 'g-', linewidth=2)\nplt.title('Feature Variance per Dimension')\nplt.xlabel('Feature Dimension')\nplt.ylabel('Variance')\nplt.grid(True, alpha=0.3)\n\n# Feature correlation matrix (subset)\nplt.subplot(1, 3, 3)\ncorrelation_matrix = np.corrcoef(features[:, :32].T)  # First 32 dimensions\nplt.imshow(correlation_matrix, cmap='coolwarm', vmin=-1, vmax=1)\nplt.title('Feature Correlation Matrix\\n(First 32 dimensions)')\nplt.colorbar()\n\nplt.tight_layout()\nplt.show()\n\n# Print statistics\nprint(f\"Feature Statistics:\")\nprint(f\"Mean: {features.mean():.4f}\")\nprint(f\"Std: {features.std():.4f}\")\nprint(f\"Min: {features.min():.4f}\")\nprint(f\"Max: {features.max():.4f}\")"
  },
  {
    "objectID": "posts/2025-07-14-SwaV/2025-07-12-swav.html#key-advantages-of-swav",
    "href": "posts/2025-07-14-SwaV/2025-07-12-swav.html#key-advantages-of-swav",
    "title": "SwaV: Swapping Assignments between Views for Unsupervised Learning of Visual Features",
    "section": "Key Advantages of SwAV",
    "text": "Key Advantages of SwAV\n\nNo Negative Sampling: Unlike contrastive methods, SwAV doesn’t require negative pairs\nScalability: Works well with large batch sizes and many prototypes\nMulti-scale Learning: Uses crops of different sizes for better representation learning\nBalanced Assignments: Sinkhorn-Knopp ensures balanced cluster assignments"
  },
  {
    "objectID": "posts/2025-07-14-SwaV/2025-07-12-swav.html#comparison-with-other-methods",
    "href": "posts/2025-07-14-SwaV/2025-07-12-swav.html#comparison-with-other-methods",
    "title": "SwaV: Swapping Assignments between Views for Unsupervised Learning of Visual Features",
    "section": "Comparison with Other Methods",
    "text": "Comparison with Other Methods\n\n\n\nMethod\nApproach\nKey Innovation\n\n\n\n\nSimCLR\nContrastive\nLarge batch sizes + strong augmentation\n\n\nMoCo\nContrastive\nMomentum encoder + queue\n\n\nSwAV\nClustering\nPrototype-based assignments + multi-crop\n\n\nBYOL\nNon-contrastive\nPredictor network + stop gradient"
  },
  {
    "objectID": "posts/2025-07-14-SwaV/2025-07-12-swav.html#practical-considerations",
    "href": "posts/2025-07-14-SwaV/2025-07-12-swav.html#practical-considerations",
    "title": "SwaV: Swapping Assignments between Views for Unsupervised Learning of Visual Features",
    "section": "Practical Considerations",
    "text": "Practical Considerations\n\nPrototype Initialization: Prototypes should be normalized and well-initialized\nSinkhorn Iterations: Usually 3 iterations are sufficient\nTemperature Scaling: Important for balancing assignments\nMulti-crop Ratios: Typically 2 global + 6 local crops"
  },
  {
    "objectID": "posts/2025-07-14-SwaV/2025-07-12-swav.html#future-directions",
    "href": "posts/2025-07-14-SwaV/2025-07-12-swav.html#future-directions",
    "title": "SwaV: Swapping Assignments between Views for Unsupervised Learning of Visual Features",
    "section": "Future Directions",
    "text": "Future Directions\nSwAV has inspired several important follow-up works in self-supervised learning:\n\nSeLa: Integrates SwAV-style clustering with momentum encoder updates for improved stability.\nDenseCL: Adapts SwAV principles for dense prediction tasks such as object detection and segmentation.\nSwAV+: Enhances the original SwAV with stronger augmentations and improved architectural choices.\n\nThis implementation serves as a strong foundation for understanding and experimenting with SwAV.\nFor real-world or production-level applications, consider using the official implementation, which includes robust ResNet backbones, better training schedules, and optimized performance settings."
  },
  {
    "objectID": "posts/2025-07-12-contrastive-learning/2025-07-12-SimCLR.html",
    "href": "posts/2025-07-12-contrastive-learning/2025-07-12-SimCLR.html",
    "title": "SimCLR: Simple Contrastive Learning of Visual Representations",
    "section": "",
    "text": "SimCLR (Simple Contrastive Learning of Visual Representations) is a self-supervised learning framework for learning visual representations without labels. It was introduced by Chen et al. in 2020 and has become one of the most influential methods in contrastive learning.\n\n\n\n\n\n\nNote\n\n\n\nRead the original paper: Chen, Ting, et al.\nA Simple Framework for Contrastive Learning of Visual Representations (2020)\narXiv:2002.05709\n\n\n\n\n\nContrastive Learning: Learn representations by contrasting positive and negative examples.\nData Augmentation: Create positive pairs through augmentation of the same image.\nProjection Head: Use a non-linear projection head during training.\nLarge Batch Sizes: Utilize large batch sizes for more negative examples.\n\n\n\n\n\nTake a batch of images.\nApply two different augmentations to each image (creating positive pairs).\nPass augmented images through an encoder (e.g., ResNet).\nApply a projection head to get representations.\nUse contrastive loss (NT-Xent) to pull positive pairs together and push negative pairs apart.\n\n\n\n\nSimCLR Architecture Overview\n\n\n\n# Required imports\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport random\nimport os\n\n# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')"
  },
  {
    "objectID": "posts/2025-07-12-contrastive-learning/2025-07-12-SimCLR.html#introduction",
    "href": "posts/2025-07-12-contrastive-learning/2025-07-12-SimCLR.html#introduction",
    "title": "SimCLR: Simple Contrastive Learning of Visual Representations",
    "section": "",
    "text": "SimCLR (Simple Contrastive Learning of Visual Representations) is a self-supervised learning framework for learning visual representations without labels. It was introduced by Chen et al. in 2020 and has become one of the most influential methods in contrastive learning.\n\n\n\n\n\n\nNote\n\n\n\nRead the original paper: Chen, Ting, et al.\nA Simple Framework for Contrastive Learning of Visual Representations (2020)\narXiv:2002.05709\n\n\n\n\n\nContrastive Learning: Learn representations by contrasting positive and negative examples.\nData Augmentation: Create positive pairs through augmentation of the same image.\nProjection Head: Use a non-linear projection head during training.\nLarge Batch Sizes: Utilize large batch sizes for more negative examples.\n\n\n\n\n\nTake a batch of images.\nApply two different augmentations to each image (creating positive pairs).\nPass augmented images through an encoder (e.g., ResNet).\nApply a projection head to get representations.\nUse contrastive loss (NT-Xent) to pull positive pairs together and push negative pairs apart.\n\n\n\n\nSimCLR Architecture Overview\n\n\n\n# Required imports\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport random\nimport os\n\n# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')"
  },
  {
    "objectID": "posts/2025-07-12-contrastive-learning/2025-07-12-SimCLR.html#data-augmentation-pipeline",
    "href": "posts/2025-07-12-contrastive-learning/2025-07-12-SimCLR.html#data-augmentation-pipeline",
    "title": "SimCLR: Simple Contrastive Learning of Visual Representations",
    "section": "Data Augmentation Pipeline",
    "text": "Data Augmentation Pipeline\nData augmentation is crucial for SimCLR’s success. The framework uses a composition of augmentations to create positive pairs from the same image.\n\n# Data Augmentation Pipeline\nclass SimCLRTransform:\n    \n    def __init__(self, image_size=224, s=1.0):\n        self.image_size = image_size\n        \n        # Color distortion\n        color_jitter = transforms.ColorJitter(\n            brightness=0.8 * s,\n            contrast=0.8 * s,\n            saturation=0.8 * s,\n            hue=0.2 * s\n        )\n        \n        # SimCLR augmentation pipeline\n        self.transform = transforms.Compose([\n            transforms.RandomResizedCrop(image_size, scale=(0.08, 1.0)),\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.RandomApply([color_jitter], p=0.8),\n            transforms.RandomGrayscale(p=0.2),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            )\n        ])\n    \n    def __call__(self, x):\n        # Return two augmented versions of the same image\n        return self.transform(x), self.transform(x)\n\n# Demonstration of augmentation\ntransform = SimCLRTransform()\nprint(\"SimCLR augmentation pipeline created\")"
  },
  {
    "objectID": "posts/2025-07-12-contrastive-learning/2025-07-12-SimCLR.html#simclr-model-architecture",
    "href": "posts/2025-07-12-contrastive-learning/2025-07-12-SimCLR.html#simclr-model-architecture",
    "title": "SimCLR: Simple Contrastive Learning of Visual Representations",
    "section": "SimCLR Model Architecture",
    "text": "SimCLR Model Architecture\nThe SimCLR model is built from two key components:\n\n1. Encoder\n\nA deep convolutional neural network (CNN), such as ResNet, is used to extract feature representations from input images.\nThe encoder learns to map images to a high-dimensional feature space that captures semantic content.\n\n\n\n2. Projection Head\n\nA small multilayer perceptron (MLP) that takes the encoder’s output and projects it into a lower-dimensional space.\nThis projection is where the contrastive loss is applied, encouraging similar images (positive pairs) to have similar representations and dissimilar images (negative pairs) to be far apart.\n\n\n\nKey Points\n\nThe projection head is used only during contrastive pre-training; for downstream tasks, only the encoder is retained.\nThis separation improves the quality of learned representations and makes SimCLR simple yet powerful for self-supervised learning.\n\n\n# SimCLR Model Architecture\nclass ProjectionHead(nn.Module):\n    \n    def __init__(self, input_dim=512, hidden_dim=512, output_dim=128):\n        super().__init__()\n        self.projection = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, output_dim)\n        )\n    \n    def forward(self, x):\n        return self.projection(x)\n\nclass SimCLR(nn.Module):\n    \"\"\"SimCLR model implementation\"\"\"\n    \n    def __init__(self, base_encoder='resnet18', projection_dim=128):\n        super().__init__()\n        \n        # Base encoder\n        if base_encoder == 'resnet18':\n            self.encoder = torchvision.models.resnet18(weights=None)\n            self.encoder.fc = nn.Identity()  # Remove classification head\n            encoder_dim = 512\n        elif base_encoder == 'resnet50':\n            self.encoder = torchvision.models.resnet50(weights=None)\n            self.encoder.fc = nn.Identity()\n            encoder_dim = 2048\n        else:\n            raise ValueError(f\"Unsupported encoder: {base_encoder}\")\n        \n        # Projection head\n        self.projection_head = ProjectionHead(\n            input_dim=encoder_dim,\n            output_dim=projection_dim\n        )\n    \n    def forward(self, x):\n        # Extract features\n        features = self.encoder(x)\n        # Project features\n        projections = self.projection_head(features)\n        return features, projections\n\n# Create model\nmodel = SimCLR(base_encoder='resnet18', projection_dim=128)\nmodel = model.to(device)\nprint(f\"SimCLR model created with {sum(p.numel() for p in model.parameters())} parameters\")"
  },
  {
    "objectID": "posts/2025-07-12-contrastive-learning/2025-07-12-SimCLR.html#nt-xent-loss-function",
    "href": "posts/2025-07-12-contrastive-learning/2025-07-12-SimCLR.html#nt-xent-loss-function",
    "title": "SimCLR: Simple Contrastive Learning of Visual Representations",
    "section": "NT-Xent Loss Function",
    "text": "NT-Xent Loss Function\nThe Normalized Temperature-scaled Cross-Entropy (NT-Xent) loss is the heart of SimCLR. It encourages similar representations for positive pairs while pushing apart negative pairs.\n\n# NT-Xent Loss Function\nclass NTXentLoss(nn.Module):\n    \n    def __init__(self, temperature=0.07):\n        super().__init__()\n        self.temperature = temperature\n        self.criterion = nn.CrossEntropyLoss(reduction='sum')\n        self.similarity_f = nn.CosineSimilarity(dim=2)\n    \n    def mask_correlated_samples(self, batch_size):\n        \"\"\"Create mask to remove self-similarity and correlated samples\"\"\"\n        N = 2 * batch_size\n        mask = torch.ones((N, N), dtype=bool)\n        mask = mask.fill_diagonal_(0)\n        for i in range(batch_size):\n            mask[i, batch_size + i] = 0\n            mask[batch_size + i, i] = 0\n        return mask\n    \n    def forward(self, z_i, z_j):\n        \"\"\"Calculate NT-Xent loss\"\"\"\n        batch_size = z_i.shape[0]\n        N = 2 * batch_size\n        z = torch.cat((z_i, z_j), dim=0)\n        sim = self.similarity_f(z.unsqueeze(1), z.unsqueeze(0)) / self.temperature\n        sim_i_j = torch.diag(sim, batch_size)\n        sim_j_i = torch.diag(sim, -batch_size)\n        positive_samples = torch.cat((sim_i_j, sim_j_i), dim=0).reshape(N, 1)\n        mask = self.mask_correlated_samples(batch_size)\n        negative_samples = sim[mask].reshape(N, -1)\n        labels = torch.zeros(N).to(positive_samples.device).long()\n        logits = torch.cat((positive_samples, negative_samples), dim=1)\n        loss = self.criterion(logits, labels)\n        return loss / N\n\n# Create loss function\ncriterion = NTXentLoss(temperature=0.07)\nprint(\"NT-Xent loss function created\")"
  },
  {
    "objectID": "posts/2025-07-12-contrastive-learning/2025-07-12-SimCLR.html#custom-dataset-for-simclr",
    "href": "posts/2025-07-12-contrastive-learning/2025-07-12-SimCLR.html#custom-dataset-for-simclr",
    "title": "SimCLR: Simple Contrastive Learning of Visual Representations",
    "section": "Custom Dataset for SimCLR",
    "text": "Custom Dataset for SimCLR\nWe’ll create a custom dataset class that applies SimCLR augmentations to create positive pairs.\n\n# Custom Dataset for SimCLR\nclass SimCLRDataset(Dataset):\n    \"\"\"Dataset wrapper for SimCLR that returns augmented pairs\"\"\"\n    \n    def __init__(self, dataset, transform=None):\n        self.dataset = dataset\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self, idx):\n        image, _ = self.dataset[idx]  \n        # Note that labels are ignored for self supervised learning\n        if self.transform:\n            aug1, aug2 = self.transform(image)\n            return aug1, aug2\n        else:\n            return image, image\n\n# Load CIFAR-10 dataset (you can replace with your own dataset)\nbase_dataset = torchvision.datasets.CIFAR10(\n    root='./data',\n    train=True,\n    download=True,\n    transform=transforms.ToPILImage()\n)\n\n# Create SimCLR dataset\nsimclr_dataset = SimCLRDataset(base_dataset, transform=SimCLRTransform(image_size=32))\ndataloader = DataLoader(simclr_dataset, batch_size=64, shuffle=True, num_workers=2)\n\nprint(f\"Dataset created with {len(simclr_dataset)} samples\")\nprint(f\"Dataloader created with batch size {dataloader.batch_size}\")"
  },
  {
    "objectID": "posts/2025-07-12-contrastive-learning/2025-07-12-SimCLR.html#training-loop",
    "href": "posts/2025-07-12-contrastive-learning/2025-07-12-SimCLR.html#training-loop",
    "title": "SimCLR: Simple Contrastive Learning of Visual Representations",
    "section": "Training Loop",
    "text": "Training Loop\nNow let’s implement the training loop for SimCLR. This demonstrates how the model learns representations through contrastive learning.\n\n# Training Loop\ndef train_simclr(model, dataloader, criterion, optimizer, num_epochs=25):\n    \"\"\"Training loop for SimCLR\"\"\"\n    model.train()\n    losses = []\n    for epoch in range(num_epochs):\n        epoch_loss = 0.0\n        num_batches = 0\n        for batch_idx, (aug1, aug2) in enumerate(dataloader):\n            aug1, aug2 = aug1.to(device), aug2.to(device)\n            _, z1 = model(aug1)\n            _, z2 = model(aug2)\n            z1 = F.normalize(z1, dim=1)\n            z2 = F.normalize(z2, dim=1)\n            loss = criterion(z1, z2)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n            num_batches += 1\n            if batch_idx % 50 == 0:\n                print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(dataloader)}, Loss: {loss.item():.4f}')\n        avg_loss = epoch_loss / num_batches\n        losses.append(avg_loss)\n        print(f'Epoch {epoch+1}/{num_epochs} completed. Average Loss: {avg_loss:.4f}')\n    return losses\n\n# Create optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-6)\n\n# Train the model \nprint(\"Starting SimCLR training\")\nlosses = train_simclr(model, dataloader, criterion, optimizer, num_epochs=25)\n\n# Plot training loss\nplt.figure(figsize=(10, 6))\nplt.plot(losses)\nplt.title('SimCLR Training Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.grid(True)\nplt.show()\n\nprint(\"Training completed\")"
  },
  {
    "objectID": "posts/2025-07-12-contrastive-learning/2025-07-12-SimCLR.html#evaluation-linear-probing",
    "href": "posts/2025-07-12-contrastive-learning/2025-07-12-SimCLR.html#evaluation-linear-probing",
    "title": "SimCLR: Simple Contrastive Learning of Visual Representations",
    "section": "Evaluation: Linear Probing",
    "text": "Evaluation: Linear Probing\nAfter pre-training with SimCLR, we typically evaluate the learned representations using linear probing—training a linear classifier on top of frozen features.\n\n# Evaluation: Linear Probing\nclass LinearProbe(nn.Module):\n    \"\"\"Linear classifier for evaluation\"\"\"\n    \n    def __init__(self, feature_dim, num_classes):\n        super().__init__()\n        self.classifier = nn.Linear(feature_dim, num_classes)\n    \n    def forward(self, x):\n        return self.classifier(x)\n\ndef evaluate_linear_probe(model, train_loader, test_loader, num_classes=10):\n    \"\"\"Evaluate learned representations using linear probing\"\"\"\n    model.eval()\n    train_features = []\n    train_labels = []\n    with torch.no_grad():\n        for images, labels in train_loader:\n            images = images.to(device)\n            features, _ = model(images)\n            train_features.append(features.cpu())\n            train_labels.append(labels)\n    train_features = torch.cat(train_features)\n    train_labels = torch.cat(train_labels)\n    linear_probe = LinearProbe(train_features.shape[1], num_classes).to(device)\n    optimizer = torch.optim.Adam(linear_probe.parameters(), lr=1e-3)\n    criterion = nn.CrossEntropyLoss()\n    linear_probe.train()\n    for epoch in range(10):\n        indices = torch.randperm(len(train_features))\n        for i in range(0, len(train_features), 256):\n            batch_indices = indices[i:i+256]\n            batch_features = train_features[batch_indices].to(device)\n            batch_labels = train_labels[batch_indices].to(device)\n            optimizer.zero_grad()\n            outputs = linear_probe(batch_features)\n            loss = criterion(outputs, batch_labels)\n            loss.backward()\n            optimizer.step()\n    linear_probe.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images = images.to(device)\n            features, _ = model(images)\n            outputs = linear_probe(features)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted.cpu() == labels).sum().item()\n    accuracy = 100 * correct / total\n    return accuracy\n\n# Create evaluation datasets\neval_transform = transforms.Compose([\n    transforms.Resize(32),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntrain_eval_dataset = torchvision.datasets.CIFAR10(\n    root='./data', train=True, download=False, transform=eval_transform\n)\ntest_eval_dataset = torchvision.datasets.CIFAR10(\n    root='./data', train=False, download=False, transform=eval_transform\n)\n\ntrain_eval_loader = DataLoader(train_eval_dataset, batch_size=256, shuffle=False)\ntest_eval_loader = DataLoader(test_eval_dataset, batch_size=256, shuffle=False)\n\n# Evaluate\nprint(\"Evaluating learned representations\")\naccuracy = evaluate_linear_probe(model, train_eval_loader, test_eval_loader)\nprint(f\"Linear probe accuracy: {accuracy:.2f}%\")"
  },
  {
    "objectID": "posts/2025-07-12-contrastive-learning/2025-07-12-SimCLR.html#visualization-feature-similarity",
    "href": "posts/2025-07-12-contrastive-learning/2025-07-12-SimCLR.html#visualization-feature-similarity",
    "title": "SimCLR: Simple Contrastive Learning of Visual Representations",
    "section": "Visualization: Feature Similarity",
    "text": "Visualization: Feature Similarity\nLet’s visualize how similar the learned features are for augmented versions of the same image.\n\n# Visualization: Feature Similarity\ndef visualize_feature_similarity(model, dataset, num_samples=5):\n    \"\"\"Visualize similarity between features of augmented pairs\"\"\"\n    model.eval()\n    fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4 * num_samples))\n    if num_samples == 1:\n        axes = axes.reshape(1, -1)\n    with torch.no_grad():\n        for i in range(num_samples):\n            aug1, aug2 = dataset[i]\n            aug1 = aug1.unsqueeze(0).to(device)\n            aug2 = aug2.unsqueeze(0).to(device)\n            _, z1 = model(aug1)\n            _, z2 = model(aug2)\n            z1 = F.normalize(z1, dim=1)\n            z2 = F.normalize(z2, dim=1)\n            similarity = F.cosine_similarity(z1, z2).item()\n            mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n            std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n            aug1_denorm = aug1.cpu().squeeze() * std + mean\n            aug2_denorm = aug2.cpu().squeeze() * std + mean\n            axes[i, 0].imshow(aug1_denorm.permute(1, 2, 0).clamp(0, 1))\n            axes[i, 0].set_title(f'Augmentation 1')\n            axes[i, 0].axis('off')\n            axes[i, 1].imshow(aug2_denorm.permute(1, 2, 0).clamp(0, 1))\n            axes[i, 1].set_title(f'Augmentation 2')\n            axes[i, 1].axis('off')\n            axes[i, 2].text(0.5, 0.5, f'Cosine Similarity:\\n{similarity:.3f}',\n                            ha='center', va='center', fontsize=16,\n                            transform=axes[i, 2].transAxes)\n            axes[i, 2].axis('off')\n    plt.tight_layout()\n    plt.show()\n\n# Visualize feature similarity\nprint(\"Visualizing feature similarity for augmented pairs\")\nvisualize_feature_similarity(model, simclr_dataset, num_samples=3)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "SimCLR: Simple Contrastive Learning of Visual Representations\n\n\n\nself-supervised\n\ncontrastive-learning\n\nvision\n\n\n\nAn in-depth look at SimCLR, a self-supervised learning framework that leverages contrastive learning to learn visual representations without labeled data.\n\n\n\n\n\nJul 12, 2025\n\n\nJeet Parab\n\n\n\n\n\n\n\n\n\n\n\n\nMomentum Contrast (MoCo)\n\n\n\nself-supervised\n\ncontrastive-learning\n\nvision\n\n\n\nUnderstanding Momentum Contrastive Learning for Unsupervised Visual Representation Learning\n\n\n\n\n\nJul 12, 2025\n\n\nJeet Parab\n\n\n\n\n\n\n\n\n\n\n\n\nSwaV: Swapping Assignments between Views for Unsupervised Learning of Visual Features\n\n\n\nself-supervised\n\ncontrastive-learning\n\nvision\n\n\n\nExploring Swapping Assignments between Views for Unsupervised Learning of Visual Features\n\n\n\n\n\nJul 12, 2025\n\n\nJeet Parab\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025-07-12-MoCo/2025-07-12-moco.html",
    "href": "posts/2025-07-12-MoCo/2025-07-12-moco.html",
    "title": "Momentum Contrast (MoCo)",
    "section": "",
    "text": "MoCo (Momentum Contrast) is a self-supervised learning framework introduced by Facebook AI Research in 2019. It addresses the fundamental challenge of learning visual representations without labeled data by treating contrastive learning as a dictionary look-up problem.\nThe key insight behind MoCo is that contrastive learning can be viewed as training an encoder to perform a dictionary look-up task, where we want to match a query representation with its corresponding key.\n\n\n\n\n\n\nNote\n\n\n\nRead the original paper: He, Kaiming, et al.\nMomentum Contrast for Unsupervised Visual Representation Learning (MoCo) (2019)\narXiv:1911.05722"
  },
  {
    "objectID": "posts/2025-07-12-MoCo/2025-07-12-moco.html#introduction",
    "href": "posts/2025-07-12-MoCo/2025-07-12-moco.html#introduction",
    "title": "Momentum Contrast (MoCo)",
    "section": "",
    "text": "MoCo (Momentum Contrast) is a self-supervised learning framework introduced by Facebook AI Research in 2019. It addresses the fundamental challenge of learning visual representations without labeled data by treating contrastive learning as a dictionary look-up problem.\nThe key insight behind MoCo is that contrastive learning can be viewed as training an encoder to perform a dictionary look-up task, where we want to match a query representation with its corresponding key.\n\n\n\n\n\n\nNote\n\n\n\nRead the original paper: He, Kaiming, et al.\nMomentum Contrast for Unsupervised Visual Representation Learning (MoCo) (2019)\narXiv:1911.05722"
  },
  {
    "objectID": "posts/2025-07-12-MoCo/2025-07-12-moco.html#core-concepts-of-moco",
    "href": "posts/2025-07-12-MoCo/2025-07-12-moco.html#core-concepts-of-moco",
    "title": "Momentum Contrast (MoCo)",
    "section": "Core Concepts of MoCo",
    "text": "Core Concepts of MoCo\n\n1. Dictionary Look-up Perspective\nMoCo (Momentum Contrast) reframes contrastive learning as a dictionary look-up task:\n\nQuery (q): An encoded representation of an augmented image.\nPositive Key (k⁺): The encoded representation of a different augmentation of the same image as the query.\nNegative Keys (k⁻): Encoded representations of different images.\nDictionary: A dynamic set of keys (representations) used to compare against the query. The goal is to bring the query closer to its positive key while pushing it away from negative keys.\n\n\n\n2. Queue Mechanism\nMoCo introduces a queue-based dictionary to efficiently manage a large set of negative samples:\n\nA FIFO queue stores encoded representations (keys) from previous mini-batches.\nAs new keys are added to the queue, the oldest keys are removed.\nThis design ensures:\n\nA large and consistent dictionary size independent of the mini-batch size.\nBetter utilization of past samples, improving contrastive learning.\n\n\n\n\n3. Momentum Update\nInstead of training both encoders via backpropagation, MoCo stabilizes learning with a momentum update for the key encoder:\n\nQuery Encoder (f_q): Updated normally using gradient descent.\nKey Encoder (f_k): Updated as an exponential moving average of the query encoder:\n\\[\n\\theta_k \\leftarrow m \\cdot \\theta_k + (1 - m) \\cdot \\theta_q\n\\]\nMomentum Coefficient (m): Typically set to 0.999, ensuring slow, stable updates.\n\nThis strategy helps maintain consistent representations for keys, reducing noise in the contrastive learning process."
  },
  {
    "objectID": "posts/2025-07-12-MoCo/2025-07-12-moco.html#moco-architecture-and-algorithm",
    "href": "posts/2025-07-12-MoCo/2025-07-12-moco.html#moco-architecture-and-algorithm",
    "title": "Momentum Contrast (MoCo)",
    "section": "MoCo Architecture and Algorithm",
    "text": "MoCo Architecture and Algorithm\n\nArchitecture Components\n\nQuery Encoder (f_q): A CNN (typically ResNet) that encodes query images.\nKey Encoder (f_k): A CNN with identical architecture to f_q, updated via momentum.\nQueue: A memory bank storing encoded keys from previous batches.\nProjection Head: An MLP that projects features into a lower-dimensional embedding space.\n\n\n\nTraining Process\n\nSample a mini-batch of N images.\nApply data augmentation to each image to create query and key views.\nEncode queries using f_q and keys using f_k.\nCompute the contrastive loss between each query and all keys in the queue.\nUpdate the query encoder (f_q) via gradient descent.\nUpdate the key encoder (f_k) via momentum update:\n\\[\n\\theta_k \\leftarrow m \\cdot \\theta_k + (1 - m) \\cdot \\theta_q\n\\]\nUpdate the queue by enqueuing the new keys and dequeuing the oldest keys.\n\n\n\nInfoNCE Loss\nMoCo uses the InfoNCE (Noise Contrastive Estimation) loss:\n\\[\n\\mathcal{L}_q = -\\log \\left( \\frac{\\exp(q \\cdot k^+ / \\tau)}{\\sum_i \\exp(q \\cdot k_i / \\tau)} \\right)\n\\]\nWhere:\n\n\\(q\\): Query representation\n\n\\(k^+\\): Positive key representation\n\n\\(k_i\\): All keys in the dictionary (including positive and negatives)\n\n\\(\\tau\\): Temperature parameter\n\n\\(\\cdot\\): Dot product (cosine similarity after L2 normalization)\n\n\n\n\nMoCo Architecture\n\n\n\n# MoCo Implementation in PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MoCo(nn.Module):\n    def __init__(self, base_encoder, dim=128, K=65536, m=0.999, T=0.07):\n        \"\"\"\n        MoCo: Momentum Contrast for Unsupervised Visual Representation Learning\n\n        Args:\n            base_encoder: backbone CNN architecture (ResNet)\n            dim: feature dimension for contrastive learning\n            K: queue size (number of negative samples)\n            m: momentum coefficient for key encoder update\n            T: temperature parameter for InfoNCE loss\n        \"\"\"\n        super(MoCo, self).__init__()\n        self.K = K\n        self.m = m\n        self.T = T\n\n        # Create query and key encoders\n        self.encoder_q = base_encoder(num_classes=dim)\n        self.encoder_k = base_encoder(num_classes=dim)\n\n        # Initialize key encoder parameters with query encoder\n        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n            param_k.data.copy_(param_q.data)\n            param_k.requires_grad = False  # Key encoder is not updated by gradient\n\n        # Create the queue for storing keys\n        self.register_buffer(\"queue\", torch.randn(dim, K))\n        self.queue = nn.functional.normalize(self.queue, dim=0)\n\n        # Queue pointer for circular buffer\n        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))\n\n    @torch.no_grad()\n    def _momentum_update_key_encoder(self):\n        \"\"\"\n        Momentum update of the key encoder\n\n        \"\"\"\n        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n            param_k.data = param_k.data * self.m + param_q.data * (1. - self.m)\n\n    @torch.no_grad()\n    def _dequeue_and_enqueue(self, keys):\n        \"\"\"\n        Update the queue by dequeuing old keys and enqueuing new keys\n        \"\"\"\n        batch_size = keys.shape[0]\n        ptr = int(self.queue_ptr)\n        assert self.K % batch_size == 0\n        self.queue[:, ptr:ptr + batch_size] = keys.transpose(0, 1)\n        ptr = (ptr + batch_size) % self.K  # Move pointer\n        self.queue_ptr[0] = ptr\n\n    @torch.no_grad()\n    def _batch_shuffle_ddp(self, x):\n        \"\"\"\n        Batch shuffle for distributed training\n        This prevents information leakage between query and key encoders\n        \"\"\"\n        batch_size_this = x.shape[0]\n        x_gather = concat_all_gather(x)\n        batch_size_all = x_gather.shape[0]\n        num_gpus = batch_size_all // batch_size_this\n        idx_shuffle = torch.randperm(batch_size_all).cuda()\n        torch.distributed.broadcast(idx_shuffle, src=0)\n        idx_unshuffle = torch.argsort(idx_shuffle)\n        gpu_idx = torch.distributed.get_rank()\n        idx_this = idx_shuffle.view(num_gpus, -1)[gpu_idx]\n        return x_gather[idx_this], idx_unshuffle\n\n    @torch.no_grad()\n    def _batch_unshuffle_ddp(self, x, idx_unshuffle):\n        \"\"\"\n        Undo batch shuffle for distributed training\n        \"\"\"\n        batch_size_this = x.shape[0]\n        x_gather = concat_all_gather(x)\n        batch_size_all = x_gather.shape[0]\n        num_gpus = batch_size_all // batch_size_this\n        gpu_idx = torch.distributed.get_rank()\n        idx_this = idx_unshuffle.view(num_gpus, -1)[gpu_idx]\n        return x_gather[idx_this]\n\n    def forward(self, im_q, im_k):\n        \"\"\"\n        Forward pass for MoCo\n\n        Args:\n            im_q: query images\n            im_k: key images\n\n        Returns:\n            logits: logits for InfoNCE loss\n            labels: labels for InfoNCE loss\n        \"\"\"\n        # Compute query features\n        q = self.encoder_q(im_q)  # queries: NxC\n        q = nn.functional.normalize(q, dim=1)\n\n        # Compute key features\n        with torch.no_grad():  # No gradient for key encoder\n            self._momentum_update_key_encoder()  # Update key encoder\n            im_k, idx_unshuffle = self._batch_shuffle_ddp(im_k)\n            k = self.encoder_k(im_k)  # keys: NxC\n            k = nn.functional.normalize(k, dim=1)\n            k = self._batch_unshuffle_ddp(k, idx_unshuffle)\n\n        # Compute logits\n        l_pos = torch.einsum('nc,nc-&gt;n', [q, k]).unsqueeze(-1)\n        l_neg = torch.einsum('nc,ck-&gt;nk', [q, self.queue.clone().detach()])\n        logits = torch.cat([l_pos, l_neg], dim=1)\n        logits /= self.T\n        labels = torch.zeros(logits.shape[0], dtype=torch.long).cuda()\n        self._dequeue_and_enqueue(k)\n        return logits, labels\n\n# Utility function for distributed training\n@torch.no_grad()\ndef concat_all_gather(tensor):\n    \"\"\"\n    Performs all_gather operation on the provided tensors\n    \"\"\"\n    tensors_gather = [torch.ones_like(tensor) for _ in range(torch.distributed.get_world_size())]\n    torch.distributed.all_gather(tensors_gather, tensor, async_op=False)\n    output = torch.cat(tensors_gather, dim=0)\n    return output"
  },
  {
    "objectID": "posts/2025-07-12-MoCo/2025-07-12-moco.html#training-loop-implementation",
    "href": "posts/2025-07-12-MoCo/2025-07-12-moco.html#training-loop-implementation",
    "title": "Momentum Contrast (MoCo)",
    "section": "Training Loop Implementation",
    "text": "Training Loop Implementation\n\n# Example usage\ndef main():\n    import torchvision.models as models\n\n    # Create ResNet-50 base encoder\n    def resnet50_encoder(num_classes=128):\n        model = models.resnet50(pretrained=False)\n        model.fc = nn.Sequential(\n            nn.Linear(model.fc.in_features, model.fc.in_features),\n            nn.ReLU(),\n            nn.Linear(model.fc.in_features, num_classes)\n        )\n        return model\n\n    # Initialize MoCo model\n    model = MoCo(resnet50_encoder, dim=128, K=65536, m=0.999, T=0.07)\n\n    # Setup optimizer \n    optimizer = torch.optim.SGD(model.parameters(), lr=0.03, momentum=0.9, weight_decay=1e-4)\n\n    # Training loop\n    for epoch in range(200):\n        train_moco(model, train_loader, optimizer, epoch, device)\n        # Add validation and checkpointing as needed\n\n\n# Data augmentation typically used with MoCo\ndef get_moco_augmentation():\n    from torchvision import transforms\n    # MoCo v1 augmentation\n    augmentation = transforms.Compose([\n        transforms.RandomResizedCrop(224, scale=(0.2, 1.0)),\n        transforms.RandomGrayscale(p=0.2),\n        transforms.ColorJitter(0.4, 0.4, 0.4, 0.4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    return augmentation"
  },
  {
    "objectID": "posts/2025-07-12-MoCo/2025-07-12-moco.html#moco-vs-simclr",
    "href": "posts/2025-07-12-MoCo/2025-07-12-moco.html#moco-vs-simclr",
    "title": "Momentum Contrast (MoCo)",
    "section": "MoCo vs SimCLR",
    "text": "MoCo vs SimCLR\n\n\n\n\n\n\n\n\nAspect\nMoCo\nSimCLR\n\n\n\n\n1. Dictionary Management\nQueue-based dictionary with large, consistent sizeIndependent of batch sizeMemory efficient\nUses within-batch negatives onlyRequires large batch sizesMemory intensive\n\n\n2. Encoder Architecture\nTwo encoders: query (f_q) and key (f_k)Momentum update: \\[\\theta_k \\leftarrow m \\cdot \\theta_k + (1 - m) \\cdot \\theta_q\\]Asymmetric design\nSingle encoder for all samplesSymmetric designNo momentum update\n\n\n3. Training Dynamics\nStable training with momentumDiverse negatives via queueRobust to batch size\nRequires large batchesAll samples updated togetherMore sensitive to batch size\n\n\n4. Computational Requirements\nLower memory footprintEfficient for small batchesWorks on modest hardware\nHigh memory requirementsNeeds multiple GPUsHeavy batch computations\n\n\n5. Augmentation Strategy\nInitially simple augmentationsMoCo v2 adopts stronger onesLess dependent on augmentation\nStrong augmentations essentialUses heavy transforms (blur, color distortions)Performance depends on augmentation strength"
  },
  {
    "objectID": "posts/2025-07-12-MoCo/2025-07-12-moco.html#summary-and-practical-recommendations",
    "href": "posts/2025-07-12-MoCo/2025-07-12-moco.html#summary-and-practical-recommendations",
    "title": "Momentum Contrast (MoCo)",
    "section": "Summary and Practical Recommendations",
    "text": "Summary and Practical Recommendations\n\nWhen to Choose MoCo:\n\nLimited computational resources\nSuitable for academic research or prototyping environments\nPreferred when stable training is important\nFlexible with varying batch sizes\nEnables faster experimentation cycles\n\n\n\nWhen to Choose SimCLR:\n\nAbundant computational resources\nIdeal for production environments with large-scale data\nNeeded when maximum performance is a priority\nWell-suited for large-scale industrial applications\nWorks best when strong augmentation pipelines are already established\n\n\n\nKey Takeaways:\n\nMoCo democratizes contrastive learning by making it accessible with limited resources\n\nSimCLR achieves strong performance but requires significant computational investment\n\nThe two methods are complementary, serving different use cases\n\nMoCo’s queue mechanism is an efficient solution to the negative sampling problem\n\nSimCLR’s simplicity makes it easier to understand and adapt to specific applications\n\nThe choice between MoCo and SimCLR depends on your available resources and performance needs. MoCo strikes a practical balance between efficiency and effectiveness, while SimCLR excels when compute and scale are not limiting factors."
  }
]